# 前言

时至今日，我们常用的计算机程序几乎都是软件开发人员从零编写的。 比如，现在我们要编写一个程序来管理网上商城。 经过思考，我们可能提出如下一个解决方案： 首先，用户通过Web浏览器（或移动应用程序）与应用程序进行交互。 紧接着，应用程序与数据库引擎进行交互，以保存交易历史记录并跟踪每个用户的动态。 其中，这个程序的核心——“业务逻辑”，详细说明了程序在各种情况下进行的操作。

为了完善业务逻辑，我们必须细致地考虑应用程序所有可能遇到的边界情况，并为这些边界情况设计合适的规则。 当买家单击将商品添加到购物车时，我们会向购物车数据库表中添加一个条目，将该用户ID与商品ID关联起来。 虽然一次编写出完美应用程序的可能性微乎其微，但在大多数情况下，我们可以从基本原理出发编写这样的程序，并不断测试直到满足用户的需求。 我们能够根据第一原则设计自动化系统，驱动正常运行的产品和系统，是一个人类认知上的非凡壮举。

幸运的是，对于日益壮大机器学习科学家群体来说，实现很多任务的自动化并不再屈从于人类的聪明才智。 想象一下，你正和你最聪明一群朋友围绕着白板，试图解决以下问题之一：

* 编写一个程序，给出地理信息、卫星图像和一些历史天气信息，来预测明天的天气。
* 编写一个程序，给出自然文本表示的问题，并正确回答该问题。
* 编写一个程序，给出一张图像，识别出图像所包含的人，并在每个人周围绘制轮廓。
* 编写一个程序，向用户推荐他们可能喜欢但在自然浏览过程中不太可能遇到的产品。

在这些情况下，即使是顶级程序员也无法从零开始，交上完美的解决方案。 原因可能各不相同：有时，我们的任务可能遵循一种随着时间推移而变化的模式，我们需要程序来自动调整。 有时，任务内的关系（比如像素和抽象类别之间的关系）可能太复杂，需要数千或数百万次的计算。 即使我们的眼睛能毫不费力地完成任务，这些计算也超出了我们的意识理解。 *机器学习*（machine learning，ML）是强大的可以从经验中学习的技术。 通常采用观测数据或与环境交互的形式，机器学习算法会积累更多的经验，其性能也会逐步提高。 相反，对比刚刚所说的电子商务平台，一直执行相同的业务逻辑，无论积累多少经验，都不会自动提高（直到开发人员认识到并更新软件）。 在这本书中，我们将带你开启机器学习之旅，并特别关注*深度学习*（deep learning）的基础知识。 这是一套强大的技术，它可以推动计算机视觉、自然语言处理、医疗保健和基因组学等不同领域的创新。

## 1.1. 日常生活中的机器学习

假设你正和本书的作者们一起，驱车去咖啡店。 亚历山大拿起一部iPhone，对它说道“Hey Siri”–手机的语音识别系统主动唤醒了。 接着，李沐对Siri说道“去星巴克咖啡店”–语音识别系统自动触发语音转文字功能，并启动地图应用程序来满足我们的请求。 地图应用程序在启动后确定了若干条路线：每条路线都显示了预计的通行时间…… 由此可见，机器学习渗透在生活中的方方面面，在短短几秒钟的时间里，我们与智能手机的日常互动就可以涉及几种机器学习模型。

现在，请你从基本原则出发，编写一个程序来响应一个“唤醒词“（比如“Alexa”、“小爱同学”和“Hey Siri”）。 试着用一台计算机和一个代码编辑器自己编写代码，如 **图1.1.1** 中所示。 问题看似很难解决：麦克风每秒钟将收集大约44000个样本，每个样本都是声波振幅的测量值。 如何编写程序，令其输入原始音频片段，输出{是,否}（表示该片段是否包含唤醒词）的可靠预测呢？ 如果你毫无头绪，别担心，我们也不知道如何从头开始编写这个程序。这就是我们需要机器学习的原因。

<div align=center>
<img width="600" src="../img/chapter01/1.1_koebel.jpg"/>
</div>
<center>图1.1.1 识别唤醒词</center>

显然，即使我们不知道如何编程从输入到输出的映射，人类大脑认知仍然能够执行这个任务。 换句话说，即使你不知道如何编写程序来识别单词“Alexa”，你的大脑却能够轻易识别它。 有了这一能力，我们就可以收集一个包含音频样本的巨大的*数据集*（dataset），并对包含和不包含唤醒词的样本进行标记。 通过机器学习算法，我们不需要设计一个“明确地”识别唤醒词的系统。 相反，我们定义一个灵活的程序算法，其输出由许多*参数*（parameter）决定。 然后我们使用数据集来确定当下的“最佳参数集”，这些参数通过某种性能度量来获取完成任务的最佳性能。

那么到底什么是参数呢？ 你可以把参数看作是旋钮，我们可以转动旋钮来调整程序的行为。 任一调整参数的程序后，我们称为*模型*（model）。 通过操作参数而生成的所有不同程序（输入-输出映射）的集合称为“模型族”。 使用数据集来选择参数的元程序被称为*学习算法*（learning algorithm）。

在我们开始用机器学习算法解决问题之前，我们必须精确地定义问题，确定输入和输出的性质，并选择合适的模型族。 在本例中，我们的模型接收一段音频作为*输入*（input），然后模型生成{是,否}中的*输出*（output）。 如果一切顺利，经过一番训练，模型对于“片段是否包含唤醒词“的预测通常是正确的。

现在我们的模型每次听到“Alexa”这个词时都会发出“是”的声音。 由于这里的唤醒词是任意选择的自然语言，因此我们可能需要一个足够丰富的模型族，使模型多元化。 比如，模型族的另一个模型只在听到“Hey Siri”这个词时发出“是”。 理想情况下，同一个模型族应该适合于“Alexa”识别和“Hey Siri”识别，因为它们似乎是相似的任务。 相反，如果我们想处理完全不同的输入或输出，比如从图像映射到字幕，或从英语映射到中文，我们可能需要一个完全不同的模型族。

正如你可能猜到的，如果我们只是随机设置模型参数，所以这个模型不太可能识别出“Alexa”、“Hey Siri”或任何其他单词。 在机器学习中，*学习*（learning）是一个模型的训练过程。 通过这个过程，我们可以发现正确的参数集，从而从使模型强制执行所需的行为。 换句话说，我们用数据*训练*（train）我们的模型。 如 图1.1.2 所示，训练过程通常包含如下步骤：

1.从一个随机初始化参数的模型开始，这个模型基本毫不”智能“。

2.获取一些数据样本（例如，音频片段以及对应的{是,否}标签）。

3.调整参数，使模型在这些样本中表现得更好。

4.重复第2步和第3步，直到模型在任务中的表现令你满意。

<div align=center>
<img width="600" src="../img/chapter01/1.1_koebel.jpg"/>
</div>
<center>图1.1.1 识别唤醒词</center>

总而言之，我们没有编写唤醒词识别器，而是编写了一个“学习”程序。 如果我们用一个巨大的带标签的数据集，它很可能可以“学习”识别唤醒词。 你可以将这种”通过用数据集来确定程序行为”的方法看作是“用数据编程”（programming with data）。 比如，我们可以通过向机器学习系统提供许多猫和狗的图片来设计一个“猫图检测器”。 通过这种方式，检测器最终可以学会：如果输入是猫的图片就输出一个非常大的正数，如果输入是狗的图片就会得出一个非常大的负数。 如果检测器不确定，它会输出接近于零的数…… 这个例子仅仅是机器学习常见应用的冰山一角。 而深度学习是机器学习的一个主要分支，我们稍后将对其进行更详细的解析。

## 1.2. 关键组件

首先，我们想让大家更清楚地了解一些核心组件。 无论我们遇到什么类型的机器学习问题，这些组件都将伴随我们左右：

1. 我们可以学习的*数据*（data）。
2. 如何转换数据的*模型*（model）。
3. 一个*目标函数*（objective function），用来量化模型的有效性。
4. 调整模型参数以优化目标函数的算法。

### 1.2.1.数据

毋庸置疑，如果没有数据，那么数据科学毫无用武之地。 每个数据集由一个个*样本*（example）组成，大多时候，它们遵循独立同分布(independently and identically distributed, i.i.d.)。 样本有时也叫做*数据点*（data point）或者*数据实例*（data instance），通常每个样本由一组称为*特征*（features，或*协变量*（covariates））的属性组成。 机器学习模型会根据这些属性进行预测。 在上面的监督学习问题中，要预测的是一个特殊的属性，它被称为*标签*（label，或*目标*（target））。

假设我们处理的是图像数据，每一张单独的照片即为一个样本，它的特征由每个像素数值的有序列表表示。 比如，$200×200$彩色照片由$200×200×3=120000$个数值组成，其中的“3”对应于每个空间位置的红、绿、蓝通道的强度。 再比如，对于一组医疗数据，给定一组标准的特征(如年龄、生命体征和诊断)，我们可能用此数据尝试预测患者是否会存活。

当每个样本的特征类别数量都是相同的，所以其特征向量是固定长度的，这个长度被称为数据的*维数*（dimensionality）。 固定长度的特征向量是一个方便的属性，它有助于我们量化学习大量样本。

然而，并不是所有的数据都可以用”固定长度“的向量表示。 以图像数据为例，如果它们全部来自标准显微镜设备，那么“固定长度”是可取的； 但是如果我们的图像数据来自互联网，我们不能天真的假想它们都有相同的分辨率或形状。 这时，我们可以考虑将图像裁剪成标准尺寸，但这种办法很局限，数据有丢失信息的风险。 此外，文本数据更不符合”固定长度“的要求。 考虑一下亚马逊等电子商务网站上的客户评论：有些文本数据是简短的（比如“好极了”）；有些则长篇大论。 与传统机器学习方法相比，深度学习的一个主要优势是可以处理不同长度的数据。

一般来说，我们拥有的数据越多，我们的工作就越容易。 当我们有了更多的数据，我们通常可以训练出更强大的模型，从而减少对预先设想假设的依赖。 数据集的由小变大为现代深度学习的成功奠定基础。 在没有大数据集的情况下，许多令人兴奋的深度学习模型黯然失色。 就算一些深度学习模型在小数据集上能够工作，但其效能并不比传统方法高。

请注意，仅仅拥有海量的数据是不够的，我们还需要正确的数据。 如果数据中充满了错误，或者如果数据的特征不能预测任务目标，那么模型很可能无效。 有一句古语很好地反映了这个现象：“输入的是垃圾,输出的也是垃圾。”（“Garbage in, garbage out.”） 此外，糟糕的预测性能甚至会加倍放大事态的严重性。 在一些敏感应用中，如预测性监管、简历筛选和用于贷款的风险模型，我们必须特别警惕垃圾数据的后果。 一种常见的问题来自不均衡的数据集，比如在一个有关医疗的训练数据集中，某些人群没有样本表示。 想象一下，假设你要训练一个皮肤癌识别模型，但它（在训练数据集）从未见过的黑色皮肤的人群，就会顿时束手无策。

再比如，如果用“过去的招聘决策数据”来训练一个筛选简历的模型，那么机器学习模型可能会无意中捕捉到历史残留的不公正，并将其自动化。 然而，这一切都可能在不知情的情况下发生。 因此，当数据不具有充分代表性，甚至包含了一些社会偏见时，模型就很有可能失败。

### 1.2.2. 模型

大多数机器学习会涉及到数据的转换。 比如，我们建立一个“摄取照片并预测笑脸”的系统。再比如，我们摄取一组传感器读数，并预测读数的正常与异常程度。 虽然简单的模型能够解决如上简单的问题，但本书中关注的问题超出了经典方法的极限。 深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为*深度学习*（deep learning）。 在讨论深度模型的过程中，我们也将提及一些传统方法。

### 1.2.3. 目标函数

前面，我们将机器学习介绍为“从经验中学习“。 这里所说的“学习”，是指自主提高模型完成某些任务的效能。 但是，什么才算真正的提高呢？ 在机器学习中，我们需要定义模型的优劣程度的度量，这个度量在大多数情况是“可优化”的，我们称之为*目标函数*（objective function）。 我们通常定义一个目标函数，并希望优化它到最低点。因为越低越好，所以这些函数有时被称为*损失函数*（loss function, 或cost function）。 但这只是一个惯例，你也可以取一个新的函数，优化到它的最高点。这两个函数本质上是相同的，只是翻转一下符号。

当任务为试图预测数值时，最常见的损失函数是*平方误差*（squared error），即预测值与实际值之差的平方。 当试图解决分类问题时，最常见的目标函数是最小化错误率，即预测与实际情况不符的样本比例。 有些目标函数（如平方误差）很容易被优化，有些目标（如错误率）由于不可微性或其他复杂性难以直接优化。 在这些情况下，通常会优化*替代目标*。

通常，损失函数是根据模型参数定义的，并取决于数据集。 在一个数据集上，我们通过最小化总损失来学习模型参数的最佳值。 该数据集由一些为训练而收集的样本组成，称为*训练数据集*（training dataset，或称为*训练集*（training set））。 然而，在训练数据上表现良好的模型，并不一定在“新数据集“上有同样的效能，这里的“新数据集“通常称为*测试数据集*（test dataset，或称为*测试集*（test set））。

综上所述，我们通常将可用数据集分成两部分：训练数据集用于拟合模型参数，测试数据集用于评估拟合的模型。 然后我们观察模型在这两部分数据集的效能。 你可以把”一个模型在训练数据集上的效能“想象成”一个学生在模拟考试中的分数“。 这个分数用来为一些真正的期末考试做参考，即使成绩令人鼓舞，也不能保证期末考试成功。 换言之，测试性能可能会显著偏离训练性能。 当一个模型在训练集上表现良好，但不能推广到测试集时，我们说这个模型是“过拟合”（overfitting）的。 就像在现实生活中，尽管模拟考试考得很好，真正的考试不一定百发百中。

### 1.2.4. 优化算法

一旦我们获得了一些数据源及其表示、一个模型和一个合适的损失函数，我们接下来就需要一种算法，它能够搜索出最佳参数，以最小化损失函数。 深度学习中，大多流行的优化算法通常基于一种基本方法–*梯度下降*（gradient descent）。 简而言之，在每个步骤中，梯度下降法都会检查每个参数，看看如果你仅对该参数进行少量变动，训练集损失会朝哪个方向移动。 然后，它在可以减少损失的方向上优化参数。

## 1.3. 各种机器学习问题

在机器学习的广泛应用中，唤醒词问题只是冰山一角。 在前面的例子中，只是机器学习可以解决的众多问题中的一个。 下面，我们将列出一些常见机器学习问题和应用，为之后本书的讨论做铺垫。 我们将不断引用前面提到的概念，如数据、模型和训练技术。

### 1.3.1. 监督学习

*监督学习*（supervised learning）擅长在“给定输入特征”的情况下预测标签。 每个“特征-标签”对都称为一个*样本*（example）。 有时，即使标签是未知的，样本也可以指代输入特征。 我们的目标是生成一个模型，能够将任何输入特征映射到标签，即预测。

举一个具体的例子。 假设我们需要预测患者是否会心脏病发作，那么观察结果“心脏病发作”或“心脏病没有发作”将是我们的标签。 输入特征可能是生命体征，如心率、舒张压和收缩压。

监督学习之所以发挥作用，是因为在训练参数时，我们为模型提供了一个数据集，其中每个样本都有真实的标签。 用概率论术语来说，我们希望预测“估计给定输入特征的标签”的条件概率。 虽然监督学习只是几大类机器学习问题之一，但是在工业中，大部分机器学习的成功应用都是监督学习。 这是因为在一定程度上，许多重要的任务可以清晰地描述为：在给定一组特定的可用数据的情况下，估计未知事物的概率。比如：

- 根据计算机断层扫描（CT）图像预测是否为癌症。
- 给出一个英语句子，预测正确的法语翻译。
- 根据本月的财务报告数据预测下个月股票的价格。

非正式地说，监督学习的学习过程如下所示。 首先，从已知大量数据样本中随机选取一个子集，为每个样本获取基本的真实标签。 有时，这些样本已有标签（例如，患者是否在下一年内康复？）； 有时，我们可能需要人工标记数据（例如，将图像分类）。 这些输入和相应的标签一起构成了训练数据集。 随后，我们选择有监督的学习算法，它将训练数据集作为输入，并输出一个“完成学习模型”。 最后，我们将之前没见过的样本特征放到这个“完成学习模型”中，使用模型的输出作为相应标签的预测。 整个监督学习过程在 [图1.3.1](https://zh-v2.d2l.ai/chapter_introduction/index.html#fig-supervised-learning) 中绘制。

#### 1.3.1.1. 回归

*回归*（regression）是最简单的监督学习任务之一。 比方说，假设我们有一组房屋销售数据表格，其中每行对应于每个房子，每列对应于一些相关的属性，例如房屋的面积、卧室的数量、浴室的数量以及到镇中心的步行分钟数等等。 对机器学习来说，每个样本即为一个特定的房屋，相应的特征向量将是表中的一行。 如果你住在纽约或旧金山，而且你不是亚马逊、谷歌、微软或Facebook的首席执行官，那么你家中的（平方英尺、卧室数量、浴室数量、步行距离）特征向量可能类似于：[600,1,1,60]。 然而，如果你住在匹兹堡，这个特征向量可能看起来更像[3000,4,3,10]

…… 为什么这个任务可以归类于回归问题呢？本质上是输出决定的。 假设你在市场上寻找新房子，你可能需要估计一栋房子的公平市场价值。 销售价格，即标签，是一个数值。 当标签取任意数值时，我们称之为*回归*问题。 我们的目标是生成一个模型，它的预测非常接近实际标签值。

生活中的许多问题都可归类于回归问题。 比如，预测用户对一部电影的评分可以被认为是一个回归问题。 这里一个小插曲，如果你在2009年设计了一个很棒的算法来预测电影评分，你可能会赢得[100万美元的奈飞奖](https://en.wikipedia.org/wiki/Netflix_Prize)。 再比如，预测病人在医院的住院时间也是一个回归问题。 总而言之，判断回归问题的一个很好的经验法则是，任何有关“多少”的问题很可能就是回归问题。比如：

- 这个手术需要多少小时？
- 在未来六小时，这个镇会有多少降雨量？

你可能发现，即使你以前从未使用过机器学习，可能在不经意间，你已经解决了一些回归问题。 例如，你让人修理了排水管，你的承包商花了3个小时清除污水管道中的污物，然后他寄给你一张350美元的账单。 而你的朋友雇了同一个承包商两个小时，他收到了250美元的账单。 如果有人请你估算的清理污物的费用，你可以假设承包商有一些基本费用，然后按小时收费。 如果这些假设成立，那么给出这两个数据样本，你就已经可以确定承包商的定价结构：每小时100美元，外加50美元上门服务费。 你看，在不经意间，你就已经理解并应用了线性回归的本质。

以上假设有时并不可取。 例如，如果一些差异是由于两个特征之外的几个因素造成的。 在这些情况下，我们将尝试学习最小化”预测值和实际标签值的差异“的模型。 在本书大部分章节中，我们将关注最小化平方误差损失函数。 正如我们稍后将看到的，这种损失对应于我们的数据被高斯噪声破坏的假设。

#### 1.3.1.2. 分类

虽然回归模型可以很好地解决“有多少？“的问题，但是很多问题并非如此。 例如，一家银行希望在其移动应用程序中添加支票扫描功能。 具体地说，这款应用程序需要能够自动理解照片图像中看到的文本，并将手写字符映射到已知字符之一。 这种“哪一个？”的问题叫做*分类*（classification）问题。 在*分类*问题中，我们希望模型能够预测样本属于哪个*类别*（category，正式称为*类*（class））。 例如，对于手写数字，我们可能有10类，分别数字0到9。 最简单的分类问题是只有两类，我们称之为“二元分类”。 例如，数据集可能由动物图像组成，标签可能是{猫,狗}两类

。 在回归中，我们训练一个回归函数来输出一个数值； 而在分类中，我们训练一个分类器，它的输出即为预测的类别。

然而模型怎么判断得出这种“是”或“不是”的硬分类预测呢？ 我们可以试着用概率语言来理解模型。 给定一个样本特征，我们的模型为每个可能的类分配一个概率。 比如，之前的猫狗分类例子中，分类器可能会输出图像是猫的概率为0.9。 0.9这个数字表达什么意思呢？ 我们可以这样解释：分类器90%确定图像描绘的是一只猫。 预测类别的概率的大小传达了一种模型的不确定性，我们将在后面章节中讨论其他运用不确定性概念的算法。

当我们有两个以上的类别时，我们把这个问题称为*多类分类*（multiclass classification）问题。 常见的例子包括手写字符识别 {0,1,2,...9,a,b,c,...}

。 与解决回归问题不同，分类问题的常见损失函数被称为*交叉熵*（cross-entropy），我们将在后面的章节中详细阐述。

请注意，最常见的类别不一定是你将用于决策的类别。 举个例子，假设你在后院发现了一个美丽的蘑菇，如 [图1.3.2](https://zh-v2.d2l.ai/chapter_introduction/index.html#fig-death-cap) 所示。

现在，请你训练一个毒蘑菇检测分类器，根据照片预测蘑菇是否有毒。 假设这个分类器输出 [图1.3.2](https://zh-v2.d2l.ai/chapter_introduction/index.html#fig-death-cap) 包含死帽蕈的概率是0.2。 换句话说，分类器80%确定我们的蘑菇不是死帽蕈。 尽管如此，我们也不会吃它，因为我们不值得冒20%的死亡风险。 换句话说，不确定风险的影响远远大于收益。 因此，我们需要将“预期风险”作为损失函数。 也就是说，我们需要将结果的概率乘以与之相关的收益（或伤害）。 在这种情况下，食用蘑菇造成的损失为 0.2×∞+0.8×0=∞，而丢弃蘑菇的损失为0.2×0+0.8×1=0.8。 我们的谨慎是有道理的：正如任何真菌学家都会告诉我们的那样， [图1.3.2](https://zh-v2.d2l.ai/chapter_introduction/index.html#fig-death-cap) 中的蘑菇实际上是一个死帽蕈。

## 参考文献

[1] Machinery, C. (1950). Computing machinery and intelligence-AM Turing. Mind, 59(236), 433.

[2] Hebb, D. O. (1949). The organization of behavior; a neuropsycholocigal theory. A Wiley Book in Clinical Psychology., 62-78.

[3] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1), 1929-1958.

[4] Bishop, C. M. (1995). Training with noise is equivalent to Tikhonov regularization. Neural computation, 7(1), 108-116.

[5] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.

[6] Sukhbaatar, S., Weston, J., & Fergus, R. (2015). End-to-end memory networks. In Advances in neural information processing systems (pp. 2440-2448).

[7] Reed, S., & De Freitas, N. (2015). Neural programmer-interpreters. arXiv preprint arXiv:1511.06279.

[8] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … & Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680).

[9] Zhu, J. Y., Park, T., Isola, P., & Efros, A. A. (2017). Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint.

[10] Karras, T., Aila, T., Laine, S., & Lehtinen, J. (2017). Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196.

[11] Li, M. (2017). Scaling Distributed Machine Learning with System and Algorithm Co-design (Doctoral dissertation, PhD thesis, Intel).

[12] You, Y., Gitman, I., & Ginsburg, B. Large batch training of convolutional networks. ArXiv e-prints.

[13] Jia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F., … & Chen, T. (2018). Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes. arXiv preprint arXiv:1807.11205.

[14] Xiong, W., Droppo, J., Huang, X., Seide, F., Seltzer, M., Stolcke, A., … & Zweig, G. (2017, March). The Microsoft 2016 conversational speech recognition system. In Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on (pp. 5255-5259). IEEE.

[15] Lin, Y., Lv, F., Zhu, S., Yang, M., Cour, T., Yu, K., … & Huang, T. (2010). Imagenet classification: fast descriptor coding and large-scale svm training. Large scale visual recognition challenge.

[16] Hu, J., Shen, L., & Sun, G. (2017). Squeeze-and-excitation networks. arXiv preprint arXiv:1709.01507, 7.

[17] Campbell, M., Hoane Jr, A. J., & Hsu, F. H. (2002). Deep blue. Artificial intelligence, 134(1-2), 57-83.

[18] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., … & Dieleman, S. (2016). Mastering the game of Go with deep neural networks and tree search. nature, 529(7587), 484.

[19] Brown, N., & Sandholm, T. (2017, August). Libratus: The superhuman ai for no-limit poker. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence.

[20] Canny, J. (1986). A computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence, (6), 679-698.

[21] Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60(2), 91-110.

[22] Salton, G., & McGill, M. J. (1986). Introduction to modern information retrieval.


-----------
> 注：本节与原书基本相同，为了完整性而搬运过来，[原书传送门](https://zh.d2l.ai/chapter_introduction/deep-learning-intro.html)
